{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORtgChxaeFkv"
      },
      "outputs": [],
      "source": [
        "!pip install wget\n",
        "import wget\n",
        "\n",
        "wget.download(\"https://github.com/GrandmaCan/ML/raw/main/Classification/one_piece_mini.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"one_piece_mini.zip\", \"r\") as zip_file:\n",
        "  zip_file.extractall(\"one_piece_mini\")"
      ],
      "metadata": {
        "id": "dFDjfLfUeS2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "  def __init__(self, root, train, transform=None):\n",
        "\n",
        "    if train:\n",
        "      image_root = Path(root) / \"train\"\n",
        "    else:\n",
        "      image_root = Path(root) / \"test\"\n",
        "\n",
        "    with open(Path(root) / \"classnames.txt\", \"r\") as f:\n",
        "      lines = f.readlines()\n",
        "      self.classes = [line.strip() for line in lines]\n",
        "\n",
        "    self.paths = [i for i in image_root.rglob(\"*\") if i.is_file()]\n",
        "    self.transform = transform\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    img = Image.open(self.paths[index]).convert(\"RGB\")\n",
        "    class_name = self.paths[index].parent.name\n",
        "    class_idx = self.classes.index(class_name)\n",
        "\n",
        "    if self.transform:\n",
        "      return self.transform(img), class_idx\n",
        "    else:\n",
        "      return img, class_idx\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.paths)"
      ],
      "metadata": {
        "id": "Ff5VDZlSeUvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = transforms.Compose([\n",
        "  transforms.Resize((64, 64)),\n",
        "  transforms.TrivialAugmentWide(),\n",
        "  transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "  transforms.Resize((64, 64)),\n",
        "  transforms.ToTensor()\n",
        "])"
      ],
      "metadata": {
        "id": "NGtT3EyKeXHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ImageDataset(root=\"one_piece_mini\",\n",
        "                train=True,\n",
        "                transform=train_transforms\n",
        ")\n",
        "\n",
        "test_dataset = ImageDataset(root=\"one_piece_mini\",\n",
        "                train=False,\n",
        "                transform=test_transforms\n",
        ")"
      ],
      "metadata": {
        "id": "p3lVuIVpeYuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "train_dataloader = DataLoader(dataset=train_dataset,\n",
        "                batch_size=BATCH_SIZE,\n",
        "                shuffle=True\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(dataset=test_dataset,\n",
        "                batch_size=BATCH_SIZE,\n",
        "                shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "I8F7xYosebMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "class ImageClassificationModel3(nn.Module):\n",
        "  def __init__(self, input_shape, output_shape):\n",
        "    super().__init__()\n",
        "    self.conv_block_1 = nn.Sequential(\n",
        "      nn.Conv2d(in_channels=input_shape,\n",
        "          out_channels=8,\n",
        "          kernel_size=(3, 3),\n",
        "          stride=1,\n",
        "          padding=1\n",
        "      ),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(in_channels=8,\n",
        "          out_channels=8,\n",
        "          kernel_size=(3, 3),\n",
        "          stride=1,\n",
        "          padding=1\n",
        "      ),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=(2, 2),\n",
        "            stride=2,\n",
        "            padding=0\n",
        "      )\n",
        "    )\n",
        "\n",
        "    self.conv_block_2 = nn.Sequential(\n",
        "      nn.Conv2d(in_channels=8,\n",
        "          out_channels=16,\n",
        "          kernel_size=(3, 3),\n",
        "          stride=1,\n",
        "          padding=1\n",
        "      ),\n",
        "      nn.ReLU(),\n",
        "      nn.Conv2d(in_channels=16,\n",
        "          out_channels=16,\n",
        "          kernel_size=(3, 3),\n",
        "          stride=1,\n",
        "          padding=1\n",
        "      ),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=(2, 2),\n",
        "            stride=2,\n",
        "            padding=0\n",
        "      )\n",
        "    )\n",
        "\n",
        "    self.classifier = nn.Sequential(\n",
        "      nn.Flatten(start_dim=1, end_dim=-1),\n",
        "      nn.Linear(in_features=16*16*16, out_features=output_shape)\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    x = self.conv_block_1(x)\n",
        "    x = self.conv_block_2(x)\n",
        "    x = self.classifier(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "bmo_0xr7eeLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "cJfFxnsNehzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ImageClassificationModel3(3, len(train_dataset.classes))\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "wDYXjEORekFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cost_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "wAoAhH6IemU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_fn(y_pred, y_true):\n",
        "\n",
        "  correct_num = (y_pred==y_true).sum()\n",
        "  acc = correct_num / len(y_true) * 100\n",
        "\n",
        "  return acc"
      ],
      "metadata": {
        "id": "_DEiLe9CeoDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(dataloader, model, cost_fn, optimizer, accuracy_fn, device):\n",
        "  train_cost = 0\n",
        "  train_acc = 0\n",
        "  for batch, (x, y) in enumerate(dataloader):\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    y_pred = model(x)\n",
        "\n",
        "    cost = cost_fn(y_pred, y)\n",
        "\n",
        "    train_cost += cost\n",
        "    train_acc += accuracy_fn(y_pred.argmax(dim=1), y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    cost.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "  train_cost /= len(train_dataloader)\n",
        "  train_acc /= len(train_dataloader)\n",
        "\n",
        "  print(f\"\\nTrain Cost: {train_cost:.4f}, Train Acc: {train_acc:.2f}\")\n",
        "\n",
        "\n",
        "def test_step(dataloader, model, cost_fn, accuracy_fn, device):\n",
        "  test_cost = 0\n",
        "  test_acc = 0\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    for x, y in dataloader:\n",
        "      x = x.to(device)\n",
        "      y = y.to(device)\n",
        "\n",
        "      test_pred = model(x)\n",
        "\n",
        "      test_cost += cost_fn(test_pred, y)\n",
        "      test_acc += accuracy_fn(test_pred.argmax(dim=1), y)\n",
        "\n",
        "    test_cost /= len(test_dataloader)\n",
        "    test_acc /= len(test_dataloader)\n",
        "\n",
        "  print(f\"Test Cost: {test_cost:.4f}, Test Acc: {test_acc:.2f} \\n\")"
      ],
      "metadata": {
        "id": "PJFLOW7Vep2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print(f\"Epoch: {epoch}\\n-------\")\n",
        "\n",
        "  train_step(train_dataloader, model, cost_fn, optimizer, accuracy_fn, device)\n",
        "\n",
        "  test_step(test_dataloader, model, cost_fn, accuracy_fn, device)"
      ],
      "metadata": {
        "id": "7xYBzdjPesPi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}